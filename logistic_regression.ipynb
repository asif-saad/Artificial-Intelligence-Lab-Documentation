{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture Notes on Logistic Regression\n",
        "---\n",
        "\n",
        "## 1. Introduction to Logistic Regression\n",
        "\n",
        "**Logistic Regression** is a statistical method used for modeling the relationship between a dependent variable and one or more independent variables. Unlike linear regression, which predicts continuous outcomes, logistic regression is used for predicting categorical outcomes, typically binary (e.g., yes/no, success/failure).\n",
        "\n",
        "### What is Logistic Regression?\n",
        "\n",
        "Logistic regression estimates the probability that a given input point belongs to a certain category. It uses the logistic function to squeeze the output of a linear equation between 0 and 1, making it suitable for binary classification tasks.\n",
        "\n",
        "### Why is Logistic Regression Important?\n",
        "\n",
        "1. **Binary Classification**:\n",
        "   - **Versatile Tool**: Ideal for problems where the outcome is binary, such as spam detection, disease diagnosis, and customer churn prediction.\n",
        "   - **Probabilistic Interpretation**: Provides probabilities for outcomes, allowing for nuanced decision-making.\n",
        "\n",
        "2. **Ease of Implementation**:\n",
        "   - **Simple Concept**: Builds on the familiar linear regression framework, making it easy to understand and implement.\n",
        "   - **Widely Available**: Supported by numerous statistical and machine learning libraries.\n",
        "\n",
        "3. **Interpretability**:\n",
        "   - **Clear Insights**: Coefficients indicate the direction and strength of the relationship between predictors and the outcome.\n",
        "   - **Feature Importance**: Helps in identifying significant predictors influencing the outcome.\n",
        "\n",
        "4. **Foundation for Advanced Models**:\n",
        "   - **Multiclass Extensions**: Forms the basis for more complex models like multinomial and ordinal logistic regression.\n",
        "   - **Integration with Other Techniques**: Can be combined with regularization methods and used in ensemble techniques.\n",
        "\n",
        "### Real-World Applications of Logistic Regression\n",
        "\n",
        "1. **Healthcare**:\n",
        "   - **Disease Prediction**: Estimating the probability of a patient having a particular disease based on symptoms and test results.\n",
        "   - **Patient Readmission**: Predicting the likelihood of a patient being readmitted to a hospital within a certain period.\n",
        "\n",
        "2. **Finance**:\n",
        "   - **Credit Scoring**: Assessing the probability of a borrower defaulting on a loan.\n",
        "   - **Fraud Detection**: Identifying potentially fraudulent transactions.\n",
        "\n",
        "3. **Marketing**:\n",
        "   - **Customer Churn Prediction**: Estimating the likelihood of customers leaving a service.\n",
        "   - **Response Modeling**: Predicting the probability of customers responding to a marketing campaign.\n",
        "\n",
        "4. **Social Sciences**:\n",
        "   - **Voting Behavior**: Predicting the likelihood of individuals voting for a particular candidate based on demographics.\n",
        "   - **Educational Outcomes**: Estimating the probability of students graduating based on various factors.\n",
        "\n",
        "5. **Technology**:\n",
        "   - **Spam Detection**: Classifying emails as spam or not spam.\n",
        "   - **Recommendation Systems**: Predicting user preferences and behaviors.\n",
        "\n",
        "### Simple Example to Illustrate Logistic Regression\n",
        "\n",
        "**Scenario**:\n",
        "Imagine you work for a bank and want to predict whether a customer will default on a loan based on their income and credit score. This is a binary classification problem where the outcome is either \"default\" or \"no default.\"\n",
        "\n",
        "**Data Collected**:\n",
        "\n",
        "| Customer | Income (X₁) | Credit Score (X₂) | Default (Y) |\n",
        "|----------|-------------|-------------------|-------------|\n",
        "| 1        | 50,000      | 700               | No          |\n",
        "| 2        | 60,000      | 650               | Yes         |\n",
        "| 3        | 55,000      | 720               | No          |\n",
        "| 4        | 45,000      | 600               | Yes         |\n",
        "| 5        | 70,000      | 750               | No          |\n",
        "| 6        | 65,000      | 680               | Yes         |\n",
        "| 7        | 80,000      | 800               | No          |\n",
        "| 8        | 40,000      | 580               | Yes         |\n",
        "| 9        | 90,000      | 820               | No          |\n",
        "| 10       | 35,000      | 550               | Yes         |\n",
        "\n",
        "**Objective**:\n",
        "Use logistic regression to predict whether a customer will default on a loan based on their income and credit score.\n",
        "\n",
        "**Steps**:\n",
        "\n",
        "1. **Plot the Data**:\n",
        "   - Create a scatter plot with Income on the X-axis and Credit Score on the Y-axis.\n",
        "   - Differentiate between default and non-default customers using colors or markers.\n",
        "\n",
        "2. **Fit the Logistic Regression Model**:\n",
        "   - Use logistic regression to find the best-fitting model that separates the default and non-default customers.\n",
        "   - The model will estimate the probability of default based on income and credit score.\n",
        "\n",
        "3. **Interpret the Results**:\n",
        "   - **Coefficients**: Indicate the impact of income and credit score on the probability of default.\n",
        "   - **Odds Ratios**: Show how the odds of default change with a one-unit increase in the predictor variables.\n",
        "\n",
        "4. **Make Predictions**:\n",
        "   - Use the model to predict the probability of default for new customers.\n",
        "   - Classify customers as \"default\" or \"no default\" based on a chosen probability threshold (e.g., 0.5).\n",
        "\n",
        "**Visualization**:\n",
        "The scatter plot with the logistic regression boundary will show how the model separates the two classes based on income and credit score.\n",
        "\n",
        "### Why is this Example Useful?\n",
        "\n",
        "- **Binary Outcome**: Clearly demonstrates a binary classification problem.\n",
        "- **Real-World Relevance**: Shows how logistic regression is applied in financial decision-making.\n",
        "- **Interpretability**: Allows for easy interpretation of how predictors influence the probability of default.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Logistic regression is a powerful tool for predicting categorical outcomes. Its ability to estimate probabilities makes it invaluable for decision-making in various fields. By understanding and applying logistic regression, you can build models that not only classify outcomes but also provide insights into the factors driving those outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Key Concepts\n",
        "\n",
        "Before diving into logistic regression, it's important to understand some basic terms and ideas that form the foundation of this method.\n",
        "\n",
        "### Dependent and Independent Variables\n",
        "\n",
        "- **Dependent Variable (Y)**: The outcome we want to predict or explain.\n",
        "  - *Example*: Default (Yes/No), Disease Status (Positive/Negative), Purchase Decision (Buy/Not Buy).\n",
        "  \n",
        "- **Independent Variable (X)**: The input or predictor that influences the dependent variable.\n",
        "  - *Example*: Income, Credit Score, Age, Number of Previous Purchases.\n",
        "\n",
        "**Example Scenario**:\n",
        "If you want to predict whether a customer will default on a loan based on their income and credit score, the default status is the dependent variable, and income and credit score are the independent variables.\n",
        "\n",
        "### The Logistic Function\n",
        "\n",
        "The logistic function, also known as the sigmoid function, transforms the output of a linear equation into a probability between 0 and 1. This makes it suitable for binary classification.\n",
        "\n",
        "**Logistic Function Formula**:\n",
        "\n",
        "$$\n",
        "\\text{Probability} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n)}}\n",
        "$$\n",
        "\n",
        "- **e**: The base of the natural logarithm (approximately equal to 2.71828).\n",
        "- $\\beta_0$: Intercept term.\n",
        "- $\\beta_1, \\beta_2, \\dots, \\beta_n$: Coefficients for the independent variables.\n",
        "- $X_1, X_2, \\dots, X_n$: Independent variables.\n",
        "\n",
        "\n",
        "**Graph of Logistic Function**:\n",
        "The logistic function produces an S-shaped curve that approaches 0 and 1 asymptotically. This characteristic makes it ideal for modeling probabilities.\n",
        "\n",
        "### Odds and Log-Odds\n",
        "\n",
        "Understanding odds and log-odds is crucial for interpreting logistic regression coefficients.\n",
        "\n",
        "- **Odds**:\n",
        "  - **Definition**: The ratio of the probability of an event occurring to the probability of it not occurring.\n",
        "  - **Formula**: $\\text{Odds} = \\frac{P(Y=1)}{P(Y=0)}$\n",
        "  - **Example**: For odds of 0.25, log-odds are $\\ln(0.25) \\approx -1.386$.\n",
        "\n",
        "- **Log-Odds (Logit)**:\n",
        "  - **Definition**: The natural logarithm of the odds.\n",
        "  - **Formula**: $\\text{Log-Odds} = \\ln\\left(\\frac{P(Y=1)}{P(Y=0)}\\right)$\n",
        "  - **Example**: For odds of 0.25, log-odds are $\\ln(0.25) \\approx -1.386$.\n",
        "\n",
        "**Relation to Logistic Function**:\n",
        "The logistic regression model models the log-odds as a linear combination of the independent variables.\n",
        "\n",
        "### Coefficients (Intercept and Slope)\n",
        "\n",
        "- **Intercept** ($\\beta_0$):\n",
        "  - **What It Represents**: The log-odds of the dependent variable when all independent variables are zero.\n",
        "  - **Interpretation**: In some cases, it may not have a meaningful real-world interpretation, especially if the independent variables cannot be zero.\n",
        "\n",
        "- **Slope** ($\\beta_1, \\beta_2, \\dots, \\beta_n$):\n",
        "  - **What They Represent**: The change in the log-odds of the dependent variable for a one-unit increase in the corresponding independent variable.\n",
        "  - **Interpretation**: Positive coefficients increase the probability of the event occurring, while negative coefficients decrease it.\n",
        "\n",
        "- **Example**:\n",
        "    If $\\beta_1 = 0.5$, then a one-unit increase in $X_1$ increases the log-odds of the event by 0.5. This translates to multiplying the odds by $e^{0.5} \\approx 1.65$, meaning the odds increase by 65%.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Assumptions of Logistic Regression\n",
        "\n",
        "For logistic regression to provide accurate and reliable results, certain assumptions about the data and the relationship between variables must be met. If these assumptions are violated, the results may be misleading.\n",
        "\n",
        "### 1. Binary Outcome\n",
        "\n",
        "- **Definition**: The dependent variable should be binary, meaning it has only two possible outcomes (e.g., Yes/No, 1/0).\n",
        "- **Why It Matters**: Logistic regression is specifically designed for binary classification tasks.\n",
        "- **How to Check**: Ensure that the dependent variable has exactly two distinct categories.\n",
        "\n",
        "### 2. Independence of Observations\n",
        "\n",
        "- **Definition**: Each observation should be independent of the others.\n",
        "- **Why It Matters**: Violations can lead to biased estimates and incorrect standard errors.\n",
        "- **How to Check**: Use study design methods to ensure independence. For time series data, consider using models that account for autocorrelation.\n",
        "\n",
        "### 3. No Multicollinearity\n",
        "\n",
        "- **Definition**: Independent variables should not be highly correlated with each other.\n",
        "- **Why It Matters**: High correlation between predictors can make it difficult to assess the individual effect of each variable.\n",
        "- **How to Check**: Calculate the Variance Inflation Factor (VIF) for each predictor. A VIF value greater than 5 or 10 indicates high multicollinearity.\n",
        "\n",
        "### 4. Linearity of the Logit\n",
        "\n",
        "- **Definition**: The logit (log-odds) should have a linear relationship with the independent variables.\n",
        "- **Why It Matters**: Logistic regression models the logit as a linear combination of the predictors.\n",
        "- **How to Check**: Use the Box-Tidwell test or include interaction terms between predictors and their log transformations.\n",
        "\n",
        "### 5. Large Sample Size\n",
        "\n",
        "- **Definition**: Logistic regression requires a sufficiently large sample size to provide reliable estimates.\n",
        "- **Why It Matters**: Small sample sizes can lead to overfitting and unreliable coefficient estimates.\n",
        "- **How to Check**: Ensure that the sample size is adequate, typically with at least 10 events per predictor variable.\n",
        "\n",
        "### 6. No Outliers or Influential Points\n",
        "\n",
        "- **Definition**: Data points should not unduly influence the model.\n",
        "- **Why It Matters**: Outliers can skew the results and lead to inaccurate predictions.\n",
        "- **How to Check**: Use diagnostic measures like Cook’s distance, leverage values, and standardized residuals to identify and address influential points.\n",
        "\n",
        "**Simple Tip**: Always visualize your data and perform diagnostic checks to ensure these assumptions hold before trusting your logistic regression results. If any assumptions are violated, consider data transformation, removing outliers, or using alternative modeling techniques.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Types of Logistic Regression\n",
        "\n",
        "Logistic regression can be categorized based on the nature of the dependent variable and the complexity of the relationship.\n",
        "\n",
        "### Binary Logistic Regression\n",
        "\n",
        "- **Definition**: Models the probability of a binary outcome based on one or more independent variables.\n",
        "- **Equation**: $\\text{Logit}(P) = \\ln\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n$\n",
        "- **Use Case**: When the dependent variable has two categories, such as default/no default, success/failure.\n",
        "\n",
        "**Example**:\n",
        "Predicting whether a customer will default on a loan based on income and credit score.\n",
        "\n",
        "### Multinomial Logistic Regression\n",
        "\n",
        "- **Definition**: Extends binary logistic regression to handle dependent variables with more than two categories that are nominal (no intrinsic order).\n",
        "- **Equation**: $\\text{Logit}(P_i) = \\ln\\left(\\frac{P_i}{P_K}\\right) = \\beta_{0i} + \\beta_{1i}X_1 + \\beta_{2i}X_2 + \\dots + \\beta_{ni}X_n$\n",
        "  - $P_i$: Probability of outcome $i$\n",
        "  - $P_K$: Probability of the reference outcome $K$\n",
        "- **Use Case**: When the dependent variable has multiple categories, such as types of housing (apartment, house, condo).\n",
        "\n",
        "**Example**:\n",
        "Classifying vehicles into categories like sedan, SUV, and truck based on features like engine size and fuel efficiency.\n",
        "\n",
        "### Ordinal Logistic Regression\n",
        "\n",
        "- **Definition**: Used when the dependent variable has more than two categories with a natural order but unknown spacing between categories.\n",
        "- **Equation**: $\\text{Logit}(P(Y \\leq j)) = \\alpha_j - (\\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n)$\n",
        "  - $\\alpha_j$: Threshold parameters for each category\n",
        "- **Use Case**: When the dependent variable is ordinal, such as customer satisfaction ratings (poor, fair, good, very good, excellent).\n",
        "\n",
        "\n",
        "**Example**:\n",
        "Predicting the likelihood of students achieving grades A, B, C, D, or F based on study hours and attendance.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Understanding the different types of logistic regression allows you to choose the appropriate model based on the nature of your dependent variable and the specific requirements of your analysis. Binary logistic regression is the most common, but multinomial and ordinal logistic regression extend the applicability of logistic models to more complex classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Evaluating Logistic Regression Models\n",
        "\n",
        "After building a logistic regression model, it's important to evaluate how well it performs. This helps in understanding the model's accuracy and reliability.\n",
        "\n",
        "### Accuracy\n",
        "\n",
        "- **Definition**: The proportion of correctly predicted instances out of all instances.\n",
        "- **Formula**: $\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Population}}$\n",
        "- **Interpretation**:\n",
        "  - **High Accuracy**: Indicates that the model correctly predicts a large proportion of instances.\n",
        "  - **Limitations**: Can be misleading in imbalanced datasets where one class dominates.\n",
        "\n",
        "**Simple Example**:\n",
        "If a model correctly predicts 90 out of 100 instances, its accuracy is 90%.\n",
        "\n",
        "### Confusion Matrix\n",
        "\n",
        "- **Definition**: A table that summarizes the performance of a classification model by showing the true versus predicted classifications.\n",
        "- **Components**:\n",
        "  - **True Positives (TP)**: Correctly predicted positive instances.\n",
        "  - **True Negatives (TN)**: Correctly predicted negative instances.\n",
        "  - **False Positives (FP)**: Incorrectly predicted positive instances.\n",
        "  - **False Negatives (FN)**: Incorrectly predicted negative instances.\n",
        "\n",
        "|                     | Predicted Positive | Predicted Negative |\n",
        "|---------------------|--------------------|--------------------|\n",
        "| **Actual Positive** | TP                 | FN                 |\n",
        "| **Actual Negative** | FP                 | TN                 |\n",
        "\n",
        "**Use Case**:\n",
        "Helps in calculating other evaluation metrics and understanding the types of errors the model makes.\n",
        "\n",
        "### Precision, Recall, F1-Score\n",
        "\n",
        "- **Precision**:\n",
        "  - **Definition**: The proportion of true positives out of all predicted positives.\n",
        "  - **Formula**: $\\text{Precision} = \\frac{TP}{TP + FP}$\n",
        "  - **Interpretation**: High precision indicates that the model has a low false positive rate.\n",
        "    \n",
        "- **Recall (Sensitivity)**:\n",
        "  - **Definition**: The proportion of true positives out of all actual positives.\n",
        "  - **Formula**: $\\text{Recall} = \\frac{TP}{TP + FN}$\n",
        "  - **Interpretation**: High recall indicates that the model captures most of the actual positives.\n",
        "    \n",
        "- **F1-Score**:\n",
        "  - **Definition**: The harmonic mean of precision and recall.\n",
        "  - **Formula**: $F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
        "  - **Interpretation**: Balances precision and recall, providing a single metric for model performance.\n",
        "\n",
        "**Simple Example**:\n",
        "If a model has Precision = 0.8 and Recall = 0.6, the F1-Score is:\n",
        "$F1 = 2 \\times \\frac{0.8 \\times 0.6}{0.8 + 0.6} = 0.69$\n",
        "\n",
        "\n",
        "### ROC Curve and AUC\n",
        "\n",
        "- **ROC Curve (Receiver Operating Characteristic Curve)**:\n",
        "  - **Definition**: A graphical representation of a model's diagnostic ability by plotting the True Positive Rate (Recall) against the False Positive Rate (1 - Specificity) at various threshold settings.\n",
        "  - **Interpretation**: The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the model.\n",
        "\n",
        "- **AUC (Area Under the ROC Curve)**:\n",
        "  - **Definition**: Measures the entire two-dimensional area underneath the ROC curve.\n",
        "  - **Range**: 0 to 1\n",
        "    - **AUC = 1**: Perfect model.\n",
        "    - **AUC = 0.5**: Model performs no better than random guessing.\n",
        "  - **Interpretation**: Higher AUC indicates better model performance.\n",
        "\n",
        "**Simple Example**:\n",
        "An AUC of 0.85 suggests that the model has a good ability to distinguish between the two classes.\n",
        "\n",
        "### Constructive Evaluation Strategy\n",
        "\n",
        "- **Use Multiple Metrics**: Relying solely on accuracy can be misleading, especially with imbalanced datasets. Combine accuracy with precision, recall, F1-Score, and AUC for a comprehensive evaluation.\n",
        "- **Confusion Matrix Analysis**: Examine the confusion matrix to understand the types of errors the model is making.\n",
        "- **ROC Curve Analysis**: Use ROC curves and AUC to assess the trade-off between true positive rates and false positive rates.\n",
        "- **Cross-Validation**: Implement cross-validation techniques to ensure that the model's performance is consistent across different subsets of the data.\n",
        "- **Threshold Optimization**: Adjust the decision threshold based on the specific requirements of your application (e.g., minimizing false negatives in disease diagnosis).\n",
        "\n",
        "**Simple Tip**: Always consider the context and the costs associated with different types of errors when choosing evaluation metrics and thresholds.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Sample Code: Implementing Logistic Regression in Python\n",
        "\n",
        "Let's see how to perform logistic regression using Python. We'll use two popular libraries: **Scikit-learn** and **Statsmodels**.\n",
        "\n",
        "### Using Scikit-learn\n",
        "\n",
        "**Scikit-learn** is a powerful library for machine learning tasks. It provides simple and efficient tools for data analysis and modeling.\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "\n",
        "# Sample Dataset: Income vs. Default\n",
        "data = {\n",
        "    'Income': [50000, 60000, 55000, 45000, 70000, 65000, 80000, 40000, 90000, 35000],\n",
        "    'Credit_Score': [700, 650, 720, 600, 750, 680, 800, 580, 820, 550],\n",
        "    'Default': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]  # 0: No Default, 1: Default\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define Independent Variables (X) and Dependent Variable (Y)\n",
        "X = df[['Income', 'Credit_Score']]\n",
        "Y = df['Default']\n",
        "\n",
        "# Split the dataset into Training and Testing sets (80% train, 20% test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "class_report = classification_report(Y_test, Y_pred)\n",
        "\n",
        "# Calculate ROC Curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "print(f\"AUC: {roc_auc:.2f}\")\n",
        "\n",
        "# Visualize the ROC Curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Visualize the Decision Boundary (if applicable)\n",
        "# Note: With two features, we can plot the decision boundary\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Define the mesh grid\n",
        "h = .02  # step size in the mesh\n",
        "x_min, x_max = X['Income'].min() - 1000, X['Income'].max() + 1000\n",
        "y_min, y_max = X['Credit_Score'].min() - 20, X['Credit_Score'].max() + 20\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Predict classifications for each point in the mesh\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Define custom colors\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
        "\n",
        "# Plot the contour and training points\n",
        "plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "# Plot also the test points\n",
        "plt.scatter(X_test['Income'], X_test['Credit_Score'], c=Y_test, cmap=cmap_bold, edgecolor='k', s=100)\n",
        "plt.xlabel('Income')\n",
        "plt.ylabel('Credit Score')\n",
        "plt.title('Logistic Regression Decision Boundary')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bQEkLFL_0mK4"
      }
    }
  ]
}